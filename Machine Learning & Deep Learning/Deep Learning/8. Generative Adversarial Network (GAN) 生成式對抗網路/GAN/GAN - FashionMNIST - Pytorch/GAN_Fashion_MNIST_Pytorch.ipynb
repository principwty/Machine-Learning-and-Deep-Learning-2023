{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/principwty/Machine-Learning-and-Deep-Learning/blob/main/GAN_Fashion_MNIST_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIl5grCmOvro"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Define the hyperparameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "lr = 0.0002\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "latent_size = 100\n",
        "image_size = 28 * 28\n",
        "hidden_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformations for the Fashion MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "y0q-fVOTPDzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generator network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, image_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "sbMqsMqvPH3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the discriminator network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(image_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "3eMn_6NPPLl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the networks\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=lr)\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "r2Jf7posPNZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the GAN\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        batch_size = images.shape[0]\n",
        "        images = images.view(batch_size, -1).to(device)\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train the discriminator with real images\n",
        "        outputs = discriminator(images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        # Train the discriminator with fake images\n",
        "        noise = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = generator(noise)\n",
        "        outputs = discriminator(fake_images)\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "        # Backpropagation and Optimization\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        discriminator.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Train the generator\n",
        "        noise = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = generator(noise)\n",
        "        outputs = discriminator(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        generator.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # Print the loss and save the generated images\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}'\n",
        "                  .format(epoch+1, num_epochs, i+1, len(train_loader), d_loss.item(), g_loss.item()))\n",
        "\n",
        "            # Save the generated images\n",
        "            fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
        "            save_image(fake_images, 'generated_images-{}.png'.format(epoch+1))\n"
      ],
      "metadata": {
        "id": "aftyOmBhPSJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea1bac9-f744-4b03-b7a6-466eb6b7ffeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step [1/469], d_loss: 1.3872, g_loss: 0.7056\n",
            "Epoch [1/50], Step [101/469], d_loss: 0.3411, g_loss: 1.8949\n",
            "Epoch [1/50], Step [201/469], d_loss: 0.1330, g_loss: 2.7505\n",
            "Epoch [1/50], Step [301/469], d_loss: 0.0905, g_loss: 3.8671\n",
            "Epoch [1/50], Step [401/469], d_loss: 0.1159, g_loss: 3.7903\n",
            "Epoch [2/50], Step [1/469], d_loss: 0.0508, g_loss: 4.2418\n",
            "Epoch [2/50], Step [101/469], d_loss: 0.0734, g_loss: 3.4324\n",
            "Epoch [2/50], Step [201/469], d_loss: 0.0814, g_loss: 3.5016\n",
            "Epoch [2/50], Step [301/469], d_loss: 0.0454, g_loss: 4.1276\n",
            "Epoch [2/50], Step [401/469], d_loss: 0.0196, g_loss: 5.0892\n",
            "Epoch [3/50], Step [1/469], d_loss: 0.0549, g_loss: 4.9826\n",
            "Epoch [3/50], Step [101/469], d_loss: 0.0178, g_loss: 5.3346\n",
            "Epoch [3/50], Step [201/469], d_loss: 0.0312, g_loss: 5.5372\n",
            "Epoch [3/50], Step [301/469], d_loss: 0.0492, g_loss: 5.2041\n",
            "Epoch [3/50], Step [401/469], d_loss: 0.0110, g_loss: 5.2085\n",
            "Epoch [4/50], Step [1/469], d_loss: 0.0062, g_loss: 7.3365\n",
            "Epoch [4/50], Step [101/469], d_loss: 0.1721, g_loss: 6.3662\n",
            "Epoch [4/50], Step [201/469], d_loss: 0.0896, g_loss: 7.1692\n",
            "Epoch [4/50], Step [301/469], d_loss: 0.2079, g_loss: 6.1533\n",
            "Epoch [4/50], Step [401/469], d_loss: 0.0613, g_loss: 6.7515\n",
            "Epoch [5/50], Step [1/469], d_loss: 0.1001, g_loss: 4.4622\n",
            "Epoch [5/50], Step [101/469], d_loss: 0.0675, g_loss: 7.2393\n",
            "Epoch [5/50], Step [201/469], d_loss: 0.1179, g_loss: 5.7893\n",
            "Epoch [5/50], Step [301/469], d_loss: 0.0539, g_loss: 7.7184\n",
            "Epoch [5/50], Step [401/469], d_loss: 0.0584, g_loss: 6.4585\n",
            "Epoch [6/50], Step [1/469], d_loss: 0.0524, g_loss: 5.1433\n",
            "Epoch [6/50], Step [101/469], d_loss: 0.0732, g_loss: 5.4875\n",
            "Epoch [6/50], Step [201/469], d_loss: 0.0685, g_loss: 6.0181\n",
            "Epoch [6/50], Step [301/469], d_loss: 0.0494, g_loss: 8.1964\n",
            "Epoch [6/50], Step [401/469], d_loss: 0.1110, g_loss: 4.3792\n",
            "Epoch [7/50], Step [1/469], d_loss: 0.1268, g_loss: 5.7311\n",
            "Epoch [7/50], Step [101/469], d_loss: 0.0791, g_loss: 5.3062\n",
            "Epoch [7/50], Step [201/469], d_loss: 0.0214, g_loss: 5.5341\n",
            "Epoch [7/50], Step [301/469], d_loss: 0.2451, g_loss: 5.4300\n",
            "Epoch [7/50], Step [401/469], d_loss: 0.0571, g_loss: 5.0045\n",
            "Epoch [8/50], Step [1/469], d_loss: 0.1146, g_loss: 5.5433\n",
            "Epoch [8/50], Step [101/469], d_loss: 0.0873, g_loss: 4.8218\n",
            "Epoch [8/50], Step [201/469], d_loss: 0.1230, g_loss: 3.4029\n",
            "Epoch [8/50], Step [301/469], d_loss: 0.2049, g_loss: 5.6368\n",
            "Epoch [8/50], Step [401/469], d_loss: 0.1259, g_loss: 5.7205\n",
            "Epoch [9/50], Step [1/469], d_loss: 0.1678, g_loss: 5.0142\n",
            "Epoch [9/50], Step [101/469], d_loss: 0.2267, g_loss: 5.3561\n",
            "Epoch [9/50], Step [201/469], d_loss: 0.4978, g_loss: 8.6200\n",
            "Epoch [9/50], Step [301/469], d_loss: 0.2607, g_loss: 5.3203\n",
            "Epoch [9/50], Step [401/469], d_loss: 0.1783, g_loss: 4.4725\n",
            "Epoch [10/50], Step [1/469], d_loss: 0.1538, g_loss: 5.2985\n",
            "Epoch [10/50], Step [101/469], d_loss: 0.0871, g_loss: 5.0881\n",
            "Epoch [10/50], Step [201/469], d_loss: 0.0956, g_loss: 4.8783\n",
            "Epoch [10/50], Step [301/469], d_loss: 0.2072, g_loss: 4.2203\n",
            "Epoch [10/50], Step [401/469], d_loss: 0.1041, g_loss: 4.6771\n",
            "Epoch [11/50], Step [1/469], d_loss: 0.2265, g_loss: 4.3110\n",
            "Epoch [11/50], Step [101/469], d_loss: 0.2937, g_loss: 5.7546\n",
            "Epoch [11/50], Step [201/469], d_loss: 0.2343, g_loss: 3.9742\n",
            "Epoch [11/50], Step [301/469], d_loss: 0.2103, g_loss: 4.1241\n",
            "Epoch [11/50], Step [401/469], d_loss: 0.1044, g_loss: 4.5718\n",
            "Epoch [12/50], Step [1/469], d_loss: 0.1683, g_loss: 4.1091\n",
            "Epoch [12/50], Step [101/469], d_loss: 0.2633, g_loss: 5.7781\n",
            "Epoch [12/50], Step [201/469], d_loss: 0.1689, g_loss: 3.9499\n",
            "Epoch [12/50], Step [301/469], d_loss: 0.2357, g_loss: 3.9785\n",
            "Epoch [12/50], Step [401/469], d_loss: 0.2076, g_loss: 4.4579\n",
            "Epoch [13/50], Step [1/469], d_loss: 0.2843, g_loss: 4.3283\n",
            "Epoch [13/50], Step [101/469], d_loss: 0.2745, g_loss: 4.0260\n",
            "Epoch [13/50], Step [201/469], d_loss: 0.3146, g_loss: 4.6022\n",
            "Epoch [13/50], Step [301/469], d_loss: 0.2821, g_loss: 4.5108\n",
            "Epoch [13/50], Step [401/469], d_loss: 0.2726, g_loss: 4.3460\n",
            "Epoch [14/50], Step [1/469], d_loss: 0.1659, g_loss: 5.1258\n",
            "Epoch [14/50], Step [101/469], d_loss: 0.2796, g_loss: 4.2507\n",
            "Epoch [14/50], Step [201/469], d_loss: 0.3397, g_loss: 4.8794\n",
            "Epoch [14/50], Step [301/469], d_loss: 0.1708, g_loss: 5.3626\n",
            "Epoch [14/50], Step [401/469], d_loss: 0.2254, g_loss: 4.2632\n",
            "Epoch [15/50], Step [1/469], d_loss: 0.2446, g_loss: 4.3122\n",
            "Epoch [15/50], Step [101/469], d_loss: 0.3649, g_loss: 3.6522\n",
            "Epoch [15/50], Step [201/469], d_loss: 0.2902, g_loss: 3.6715\n",
            "Epoch [15/50], Step [301/469], d_loss: 0.2185, g_loss: 4.4220\n",
            "Epoch [15/50], Step [401/469], d_loss: 0.2488, g_loss: 4.3049\n",
            "Epoch [16/50], Step [1/469], d_loss: 0.2259, g_loss: 3.6523\n",
            "Epoch [16/50], Step [101/469], d_loss: 0.3605, g_loss: 4.0699\n",
            "Epoch [16/50], Step [201/469], d_loss: 0.1787, g_loss: 4.2255\n",
            "Epoch [16/50], Step [301/469], d_loss: 0.2458, g_loss: 4.3054\n",
            "Epoch [16/50], Step [401/469], d_loss: 0.1537, g_loss: 4.0136\n",
            "Epoch [17/50], Step [1/469], d_loss: 0.2529, g_loss: 4.2718\n",
            "Epoch [17/50], Step [101/469], d_loss: 0.3732, g_loss: 3.3249\n",
            "Epoch [17/50], Step [201/469], d_loss: 0.5312, g_loss: 2.7946\n",
            "Epoch [17/50], Step [301/469], d_loss: 0.2710, g_loss: 4.1485\n",
            "Epoch [17/50], Step [401/469], d_loss: 0.3349, g_loss: 3.8492\n",
            "Epoch [18/50], Step [1/469], d_loss: 0.3881, g_loss: 2.9337\n",
            "Epoch [18/50], Step [101/469], d_loss: 0.2836, g_loss: 3.8902\n",
            "Epoch [18/50], Step [201/469], d_loss: 0.3230, g_loss: 3.6876\n",
            "Epoch [18/50], Step [301/469], d_loss: 0.4003, g_loss: 3.3681\n",
            "Epoch [18/50], Step [401/469], d_loss: 0.6247, g_loss: 2.6454\n",
            "Epoch [19/50], Step [1/469], d_loss: 0.4429, g_loss: 3.9706\n",
            "Epoch [19/50], Step [101/469], d_loss: 0.4387, g_loss: 3.0656\n",
            "Epoch [19/50], Step [201/469], d_loss: 0.4127, g_loss: 2.8394\n",
            "Epoch [19/50], Step [301/469], d_loss: 0.3428, g_loss: 3.6288\n",
            "Epoch [19/50], Step [401/469], d_loss: 0.3328, g_loss: 3.7189\n",
            "Epoch [20/50], Step [1/469], d_loss: 0.3661, g_loss: 3.1126\n",
            "Epoch [20/50], Step [101/469], d_loss: 0.5164, g_loss: 2.3862\n",
            "Epoch [20/50], Step [201/469], d_loss: 0.4525, g_loss: 3.4805\n",
            "Epoch [20/50], Step [301/469], d_loss: 0.3581, g_loss: 3.2008\n",
            "Epoch [20/50], Step [401/469], d_loss: 0.7432, g_loss: 3.1121\n",
            "Epoch [21/50], Step [1/469], d_loss: 0.4868, g_loss: 3.1570\n",
            "Epoch [21/50], Step [101/469], d_loss: 0.5671, g_loss: 2.5221\n",
            "Epoch [21/50], Step [201/469], d_loss: 0.3770, g_loss: 3.2202\n",
            "Epoch [21/50], Step [301/469], d_loss: 0.2920, g_loss: 3.6982\n",
            "Epoch [21/50], Step [401/469], d_loss: 0.2250, g_loss: 3.4576\n",
            "Epoch [22/50], Step [1/469], d_loss: 0.2985, g_loss: 3.2174\n",
            "Epoch [22/50], Step [101/469], d_loss: 0.2814, g_loss: 3.7394\n",
            "Epoch [22/50], Step [201/469], d_loss: 0.5210, g_loss: 3.5281\n",
            "Epoch [22/50], Step [301/469], d_loss: 0.3771, g_loss: 3.3571\n",
            "Epoch [22/50], Step [401/469], d_loss: 0.6942, g_loss: 2.6201\n",
            "Epoch [23/50], Step [1/469], d_loss: 0.2669, g_loss: 3.3996\n",
            "Epoch [23/50], Step [101/469], d_loss: 0.2486, g_loss: 3.4884\n",
            "Epoch [23/50], Step [201/469], d_loss: 0.4724, g_loss: 2.7559\n",
            "Epoch [23/50], Step [301/469], d_loss: 0.2707, g_loss: 3.1425\n",
            "Epoch [23/50], Step [401/469], d_loss: 0.2612, g_loss: 3.2388\n",
            "Epoch [24/50], Step [1/469], d_loss: 0.3001, g_loss: 4.2570\n",
            "Epoch [24/50], Step [101/469], d_loss: 0.4381, g_loss: 3.0552\n",
            "Epoch [24/50], Step [201/469], d_loss: 0.3196, g_loss: 3.8349\n",
            "Epoch [24/50], Step [301/469], d_loss: 0.2749, g_loss: 3.6586\n",
            "Epoch [24/50], Step [401/469], d_loss: 0.5570, g_loss: 3.7008\n",
            "Epoch [25/50], Step [1/469], d_loss: 0.2575, g_loss: 3.2912\n",
            "Epoch [25/50], Step [101/469], d_loss: 0.3174, g_loss: 3.1608\n",
            "Epoch [25/50], Step [201/469], d_loss: 0.4581, g_loss: 3.9902\n",
            "Epoch [25/50], Step [301/469], d_loss: 0.3122, g_loss: 3.5088\n",
            "Epoch [25/50], Step [401/469], d_loss: 0.3000, g_loss: 3.5978\n",
            "Epoch [26/50], Step [1/469], d_loss: 0.5480, g_loss: 3.6074\n",
            "Epoch [26/50], Step [101/469], d_loss: 0.2896, g_loss: 3.1622\n",
            "Epoch [26/50], Step [201/469], d_loss: 0.2729, g_loss: 4.0901\n",
            "Epoch [26/50], Step [301/469], d_loss: 0.3781, g_loss: 4.1303\n",
            "Epoch [26/50], Step [401/469], d_loss: 0.3418, g_loss: 3.3676\n",
            "Epoch [27/50], Step [1/469], d_loss: 0.5304, g_loss: 3.2863\n",
            "Epoch [27/50], Step [101/469], d_loss: 0.5905, g_loss: 3.9265\n",
            "Epoch [27/50], Step [201/469], d_loss: 0.4543, g_loss: 2.7134\n",
            "Epoch [27/50], Step [301/469], d_loss: 0.2294, g_loss: 4.3711\n",
            "Epoch [27/50], Step [401/469], d_loss: 0.1606, g_loss: 4.6915\n",
            "Epoch [28/50], Step [1/469], d_loss: 0.2945, g_loss: 3.4664\n",
            "Epoch [28/50], Step [101/469], d_loss: 0.3166, g_loss: 3.3260\n",
            "Epoch [28/50], Step [201/469], d_loss: 0.4715, g_loss: 3.1642\n",
            "Epoch [28/50], Step [301/469], d_loss: 0.3275, g_loss: 2.7224\n",
            "Epoch [28/50], Step [401/469], d_loss: 0.4344, g_loss: 2.9494\n",
            "Epoch [29/50], Step [1/469], d_loss: 0.2350, g_loss: 4.4402\n",
            "Epoch [29/50], Step [101/469], d_loss: 0.4416, g_loss: 3.0487\n",
            "Epoch [29/50], Step [201/469], d_loss: 0.3034, g_loss: 3.2840\n",
            "Epoch [29/50], Step [301/469], d_loss: 0.4699, g_loss: 3.1369\n",
            "Epoch [29/50], Step [401/469], d_loss: 0.5788, g_loss: 2.9532\n",
            "Epoch [30/50], Step [1/469], d_loss: 0.3075, g_loss: 3.8920\n",
            "Epoch [30/50], Step [101/469], d_loss: 0.2906, g_loss: 3.8064\n",
            "Epoch [30/50], Step [201/469], d_loss: 0.2147, g_loss: 3.5417\n",
            "Epoch [30/50], Step [301/469], d_loss: 0.3669, g_loss: 3.4558\n",
            "Epoch [30/50], Step [401/469], d_loss: 0.4151, g_loss: 2.9327\n",
            "Epoch [31/50], Step [1/469], d_loss: 0.3523, g_loss: 3.8118\n",
            "Epoch [31/50], Step [101/469], d_loss: 0.4018, g_loss: 3.4454\n",
            "Epoch [31/50], Step [201/469], d_loss: 0.3711, g_loss: 3.6308\n",
            "Epoch [31/50], Step [301/469], d_loss: 0.4288, g_loss: 2.7703\n",
            "Epoch [31/50], Step [401/469], d_loss: 0.4618, g_loss: 3.3177\n",
            "Epoch [32/50], Step [1/469], d_loss: 0.4355, g_loss: 3.9203\n",
            "Epoch [32/50], Step [101/469], d_loss: 0.5839, g_loss: 2.7565\n",
            "Epoch [32/50], Step [201/469], d_loss: 0.3142, g_loss: 3.6689\n",
            "Epoch [32/50], Step [301/469], d_loss: 0.4695, g_loss: 3.0902\n",
            "Epoch [32/50], Step [401/469], d_loss: 0.3258, g_loss: 3.6339\n",
            "Epoch [33/50], Step [1/469], d_loss: 0.5193, g_loss: 3.1937\n",
            "Epoch [33/50], Step [101/469], d_loss: 0.3598, g_loss: 2.9785\n",
            "Epoch [33/50], Step [201/469], d_loss: 0.3100, g_loss: 3.0897\n",
            "Epoch [33/50], Step [301/469], d_loss: 0.4173, g_loss: 4.0858\n",
            "Epoch [33/50], Step [401/469], d_loss: 0.3691, g_loss: 3.4913\n",
            "Epoch [34/50], Step [1/469], d_loss: 0.4076, g_loss: 3.4782\n",
            "Epoch [34/50], Step [101/469], d_loss: 0.4277, g_loss: 3.5982\n",
            "Epoch [34/50], Step [201/469], d_loss: 0.4480, g_loss: 2.7712\n",
            "Epoch [34/50], Step [301/469], d_loss: 0.4372, g_loss: 3.4817\n",
            "Epoch [34/50], Step [401/469], d_loss: 0.4310, g_loss: 2.7088\n",
            "Epoch [35/50], Step [1/469], d_loss: 0.5416, g_loss: 3.1462\n",
            "Epoch [35/50], Step [101/469], d_loss: 0.3685, g_loss: 3.5731\n",
            "Epoch [35/50], Step [201/469], d_loss: 0.3982, g_loss: 3.4394\n",
            "Epoch [35/50], Step [301/469], d_loss: 0.5285, g_loss: 2.5101\n",
            "Epoch [35/50], Step [401/469], d_loss: 0.5049, g_loss: 3.3987\n",
            "Epoch [36/50], Step [1/469], d_loss: 0.4527, g_loss: 2.6544\n",
            "Epoch [36/50], Step [101/469], d_loss: 0.4940, g_loss: 3.1705\n",
            "Epoch [36/50], Step [201/469], d_loss: 0.6630, g_loss: 2.9204\n",
            "Epoch [36/50], Step [301/469], d_loss: 0.5068, g_loss: 3.3524\n",
            "Epoch [36/50], Step [401/469], d_loss: 0.4134, g_loss: 4.0822\n",
            "Epoch [37/50], Step [1/469], d_loss: 0.3416, g_loss: 3.2364\n",
            "Epoch [37/50], Step [101/469], d_loss: 0.4611, g_loss: 3.1753\n",
            "Epoch [37/50], Step [201/469], d_loss: 0.3605, g_loss: 3.9543\n",
            "Epoch [37/50], Step [301/469], d_loss: 0.5467, g_loss: 2.6651\n",
            "Epoch [37/50], Step [401/469], d_loss: 0.4002, g_loss: 2.7554\n",
            "Epoch [38/50], Step [1/469], d_loss: 0.2333, g_loss: 3.7490\n",
            "Epoch [38/50], Step [101/469], d_loss: 0.3853, g_loss: 3.2891\n",
            "Epoch [38/50], Step [201/469], d_loss: 0.4838, g_loss: 3.2032\n",
            "Epoch [38/50], Step [301/469], d_loss: 0.3437, g_loss: 3.0981\n",
            "Epoch [38/50], Step [401/469], d_loss: 0.4584, g_loss: 3.4184\n",
            "Epoch [39/50], Step [1/469], d_loss: 0.4782, g_loss: 4.6668\n",
            "Epoch [39/50], Step [101/469], d_loss: 0.3885, g_loss: 4.0273\n",
            "Epoch [39/50], Step [201/469], d_loss: 0.5018, g_loss: 3.7512\n",
            "Epoch [39/50], Step [301/469], d_loss: 0.5486, g_loss: 2.8240\n",
            "Epoch [39/50], Step [401/469], d_loss: 0.3095, g_loss: 3.3754\n",
            "Epoch [40/50], Step [1/469], d_loss: 0.5497, g_loss: 2.5735\n",
            "Epoch [40/50], Step [101/469], d_loss: 0.3126, g_loss: 3.3646\n",
            "Epoch [40/50], Step [201/469], d_loss: 0.4680, g_loss: 2.6279\n",
            "Epoch [40/50], Step [301/469], d_loss: 0.4824, g_loss: 2.8579\n",
            "Epoch [40/50], Step [401/469], d_loss: 0.6128, g_loss: 3.3438\n",
            "Epoch [41/50], Step [1/469], d_loss: 0.2271, g_loss: 3.7050\n",
            "Epoch [41/50], Step [101/469], d_loss: 0.5393, g_loss: 3.1449\n",
            "Epoch [41/50], Step [201/469], d_loss: 0.7088, g_loss: 3.1197\n",
            "Epoch [41/50], Step [301/469], d_loss: 0.5818, g_loss: 3.2506\n",
            "Epoch [41/50], Step [401/469], d_loss: 0.3624, g_loss: 4.9334\n",
            "Epoch [42/50], Step [1/469], d_loss: 0.3649, g_loss: 3.8554\n",
            "Epoch [42/50], Step [101/469], d_loss: 0.3823, g_loss: 2.9975\n",
            "Epoch [42/50], Step [201/469], d_loss: 0.3160, g_loss: 4.0937\n",
            "Epoch [42/50], Step [301/469], d_loss: 0.5540, g_loss: 3.5451\n",
            "Epoch [42/50], Step [401/469], d_loss: 0.6501, g_loss: 2.5079\n",
            "Epoch [43/50], Step [1/469], d_loss: 0.3342, g_loss: 3.4487\n",
            "Epoch [43/50], Step [101/469], d_loss: 0.3870, g_loss: 2.7150\n",
            "Epoch [43/50], Step [201/469], d_loss: 0.4511, g_loss: 2.8613\n",
            "Epoch [43/50], Step [301/469], d_loss: 0.3488, g_loss: 3.2546\n",
            "Epoch [43/50], Step [401/469], d_loss: 0.4838, g_loss: 2.6829\n",
            "Epoch [44/50], Step [1/469], d_loss: 0.4558, g_loss: 2.9484\n",
            "Epoch [44/50], Step [101/469], d_loss: 0.4598, g_loss: 2.5808\n",
            "Epoch [44/50], Step [201/469], d_loss: 0.4172, g_loss: 4.1922\n",
            "Epoch [44/50], Step [301/469], d_loss: 0.6357, g_loss: 3.8068\n",
            "Epoch [44/50], Step [401/469], d_loss: 0.5252, g_loss: 3.7957\n",
            "Epoch [45/50], Step [1/469], d_loss: 0.4660, g_loss: 3.2979\n",
            "Epoch [45/50], Step [101/469], d_loss: 0.5140, g_loss: 2.6237\n",
            "Epoch [45/50], Step [201/469], d_loss: 0.5548, g_loss: 2.5330\n",
            "Epoch [45/50], Step [301/469], d_loss: 0.4897, g_loss: 3.9160\n",
            "Epoch [45/50], Step [401/469], d_loss: 0.4135, g_loss: 2.4873\n",
            "Epoch [46/50], Step [1/469], d_loss: 0.6102, g_loss: 3.0355\n",
            "Epoch [46/50], Step [101/469], d_loss: 0.4128, g_loss: 3.0332\n",
            "Epoch [46/50], Step [201/469], d_loss: 0.3200, g_loss: 2.7584\n",
            "Epoch [46/50], Step [301/469], d_loss: 0.3853, g_loss: 3.3910\n",
            "Epoch [46/50], Step [401/469], d_loss: 0.3931, g_loss: 3.2362\n",
            "Epoch [47/50], Step [1/469], d_loss: 0.4272, g_loss: 2.7324\n",
            "Epoch [47/50], Step [101/469], d_loss: 0.6174, g_loss: 3.3536\n",
            "Epoch [47/50], Step [201/469], d_loss: 0.4683, g_loss: 3.6037\n",
            "Epoch [47/50], Step [301/469], d_loss: 0.3513, g_loss: 3.4941\n",
            "Epoch [47/50], Step [401/469], d_loss: 0.3935, g_loss: 3.4151\n",
            "Epoch [48/50], Step [1/469], d_loss: 0.7103, g_loss: 2.5631\n",
            "Epoch [48/50], Step [101/469], d_loss: 0.6840, g_loss: 3.3268\n",
            "Epoch [48/50], Step [201/469], d_loss: 0.7383, g_loss: 2.3063\n",
            "Epoch [48/50], Step [301/469], d_loss: 0.3771, g_loss: 4.0898\n",
            "Epoch [48/50], Step [401/469], d_loss: 0.4332, g_loss: 2.9041\n",
            "Epoch [49/50], Step [1/469], d_loss: 0.4560, g_loss: 3.6192\n",
            "Epoch [49/50], Step [101/469], d_loss: 0.4098, g_loss: 3.2403\n",
            "Epoch [49/50], Step [201/469], d_loss: 0.5488, g_loss: 2.9931\n",
            "Epoch [49/50], Step [301/469], d_loss: 0.6561, g_loss: 2.6122\n",
            "Epoch [49/50], Step [401/469], d_loss: 0.7550, g_loss: 2.7123\n",
            "Epoch [50/50], Step [1/469], d_loss: 0.3944, g_loss: 2.9933\n",
            "Epoch [50/50], Step [101/469], d_loss: 0.4707, g_loss: 2.9076\n",
            "Epoch [50/50], Step [201/469], d_loss: 0.6185, g_loss: 3.0545\n",
            "Epoch [50/50], Step [301/469], d_loss: 0.5713, g_loss: 2.2934\n",
            "Epoch [50/50], Step [401/469], d_loss: 0.3124, g_loss: 2.7354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gvHF7D3BPbCe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}